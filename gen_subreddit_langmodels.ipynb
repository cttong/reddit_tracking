{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the_donald language model base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "col_types = {'author': str, 'body':str, 'subreddit':str, 'created_utc':int, 'score': int}\n",
    "td_df = pd.read_csv('./data/language_models/td_langsampdata_000000000000', dtype = col_types)\n",
    "td_df = td_df.append(pd.read_csv('./data/language_models/td_langsampdata_000000000001', dtype = col_types), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess Data\n",
    "td_df = td_df[td_df['subreddit'] == 'The_Donald'] #Filter by relevant subreddit\n",
    "td_df = td_df[(td_df['created_utc'] >= 1454284800) & (td_df['created_utc'] < 1468540800)] #Filter by relevant time frame \n",
    "\n",
    "td_df = td_df.sort_values(['created_utc','author']).reset_index(drop=True) #Sort Data\n",
    "td_df['body'] = td_df['body'].apply(lambda x: str(x)) #fix exceptions [was reading as NaN instead of string]\n",
    "\n",
    "#Replace open parens with space (to account for hyperlink formatting on reddit)\n",
    "td_df['body'] = td_df['body'].apply(lambda x: x.replace(r\"(\",\"  \")) \n",
    "td_df['body'] = td_df['body'].apply(lambda x: x.replace(r\")\",\"  \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-nyx-</td>\n",
       "      <td>[the, ruling, coalition, already, doesnt, have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0piat3</td>\n",
       "      <td>[do, you, think, the, black, gun, is, scarier,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000Clowns</td>\n",
       "      <td>[if, that, happens, this, republican, since, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000stomachcrunches</td>\n",
       "      <td>[well, yeah, despite, being, the, best, and, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1011011</td>\n",
       "      <td>[if, thats, the, case, she, should, be, pullin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11234a3</td>\n",
       "      <td>[yeesh, who, jerks, it, to, 3d, women, anymore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1337and0</td>\n",
       "      <td>[aw, dang, i, shouldve, changed, ricky, to, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>138glv</td>\n",
       "      <td>[god, dam, rekt, we, can, all, agree, induce, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13inchpoop</td>\n",
       "      <td>[women, are, always, the, ones, bitching, abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1875coalminer</td>\n",
       "      <td>[whoa, put, a, nsfw, tag, on, that, too, much,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author                                               body\n",
       "0                -nyx-  [the, ruling, coalition, already, doesnt, have...\n",
       "1               0piat3  [do, you, think, the, black, gun, is, scarier,...\n",
       "2           1000Clowns  [if, that, happens, this, republican, since, 1...\n",
       "3  1000stomachcrunches  [well, yeah, despite, being, the, best, and, b...\n",
       "4              1011011  [if, thats, the, case, she, should, be, pullin...\n",
       "5              11234a3  [yeesh, who, jerks, it, to, 3d, women, anymore...\n",
       "6             1337and0  [aw, dang, i, shouldve, changed, ricky, to, re...\n",
       "7               138glv  [god, dam, rekt, we, can, all, agree, induce, ...\n",
       "8           13inchpoop  [women, are, always, the, ones, bitching, abou...\n",
       "9        1875coalminer  [whoa, put, a, nsfw, tag, on, that, too, much,..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group data s.t. authors/bodies are grouped together\n",
    "td_df = td_df.groupby('author')['body'].apply(lambda x: \"%s\" % ' '.join(x)).reset_index()\n",
    "#This should result in 1 combined string per person (each person posts one \"super post\" made up of all their posts)\n",
    "\n",
    "#Import and utilize a function from Jack's script which roughly sanitizes input strings\n",
    "from fighting_words_py3 import basic_sanitize \n",
    "td_df['body'] = td_df['body'].apply(lambda x: basic_sanitize(x).split()) \n",
    "\n",
    "#Replace all strings which start with \"http\" with a marker for hyperlinks\n",
    "td_df['body'] = td_df['body'].apply(lambda x: [i if i[0:4] != 'http' else '<HYPERLINK>' for i in x])\n",
    "td_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Export data\n",
    "td_df.to_csv('./data/language_models/td_preproc_langmodel.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess sandersforpresident language model base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "sfp_df = pd.read_csv('./data/language_models/sfp_langmodeldata_000000000000')\n",
    "sdf_df = pd.read_csv('./data/language_models/sfp_langmodeldata_000000000001', dtype = col_types)\n",
    "#sfp_df = sfp_df.append(pd.read_pickle('./data/language_models/sfp_langmodeldata_000000000001.p'), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess Data\n",
    "sfp_df = sfp_df[sfp_df['subreddit'] == 'SandersForPresident'] #Filter by relevant subreddit\n",
    "sfp_df = sfp_df[(sfp_df['created_utc'] >= 1454284800) & (sfp_df['created_utc'] < 1468540800)] #Filter by relevant time frame \n",
    "\n",
    "sfp_df = sfp_df.sort_values(['created_utc','author']).reset_index(drop=True) #Sort Data\n",
    "sfp_df['body'] = sfp_df['body'].apply(lambda x: str(x)) #fix exceptions [was reading as NaN instead of string]\n",
    "\n",
    "#Replace open parens with space (to account for hyperlink formatting on reddit)\n",
    "sfp_df['body'] = sfp_df['body'].apply(lambda x: x.replace(r\"(\",\"  \")) \n",
    "sfp_df['body'] = sfp_df['body'].apply(lambda x: x.replace(r\")\",\"  \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Gaka-</td>\n",
       "      <td>[hes, gotten, voices, in, the, democratic, par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-GheeButtersnaps-</td>\n",
       "      <td>[better, her, than, the, rest, yeah, this, is,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-MOPPET-</td>\n",
       "      <td>[i, missed, the, registration, deadline, for, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-SHMOHAWK-</td>\n",
       "      <td>[yeah, please, fill, me, in, wonder, what, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-Tesserex-</td>\n",
       "      <td>[i, dont, know, why, but, when, i, read, send,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-chia-</td>\n",
       "      <td>[just, common, sense, from, bernie, here, no, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0116316</td>\n",
       "      <td>[i, dont, have, a, choice, my, work, leaves, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0826</td>\n",
       "      <td>[me, too, i, was, starting, to, think, this, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0beatbryce</td>\n",
       "      <td>[ummm, have, you, read, the, comments, people,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0m3r7a</td>\n",
       "      <td>[right, its, amazing, that, people, completely...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                                               body\n",
       "0             -Gaka-  [hes, gotten, voices, in, the, democratic, par...\n",
       "1  -GheeButtersnaps-  [better, her, than, the, rest, yeah, this, is,...\n",
       "2           -MOPPET-  [i, missed, the, registration, deadline, for, ...\n",
       "3         -SHMOHAWK-  [yeah, please, fill, me, in, wonder, what, the...\n",
       "4         -Tesserex-  [i, dont, know, why, but, when, i, read, send,...\n",
       "5             -chia-  [just, common, sense, from, bernie, here, no, ...\n",
       "6            0116316  [i, dont, have, a, choice, my, work, leaves, c...\n",
       "7               0826  [me, too, i, was, starting, to, think, this, w...\n",
       "8         0beatbryce  [ummm, have, you, read, the, comments, people,...\n",
       "9             0m3r7a  [right, its, amazing, that, people, completely..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group data s.t. authors/bodies are grouped together\n",
    "sfp_df = sfp_df.groupby('author')['body'].apply(lambda x: \"%s\" % ' '.join(x)).reset_index()\n",
    "#This should result in 1 combined string per person (each person posts one \"super post\" made up of all their posts)\n",
    "\n",
    "#Import and utilize a function from Jack's script which roughly sanitizes input strings\n",
    "from fighting_words_py3 import basic_sanitize \n",
    "sfp_df['body'] = sfp_df['body'].apply(lambda x: basic_sanitize(x).split()) \n",
    "\n",
    "#Replace all strings which start with \"http\" with a marker for hyperlinks\n",
    "sfp_df['body'] = sfp_df['body'].apply(lambda x: [i if i[0:4] != 'http' else '<HYPERLINK>' for i in x])\n",
    "sfp_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Export data\n",
    "sfp_df.to_csv('./data/language_models/sfp_preproc_langmodel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jesuslordofporn</td>\n",
       "      <td>If that is the case, I wonder if he would make...</td>\n",
       "      <td>SandersForPresident</td>\n",
       "      <td>1454297222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2gurl2gurl</td>\n",
       "      <td>The key through getting legislation through Co...</td>\n",
       "      <td>SandersForPresident</td>\n",
       "      <td>1454302379</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2gurl2gurl</td>\n",
       "      <td>They win because we sit at home unless we are ...</td>\n",
       "      <td>SandersForPresident</td>\n",
       "      <td>1454302485</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jrrl</td>\n",
       "      <td>Or, better, spend those 30 minutes meeting oth...</td>\n",
       "      <td>SandersForPresident</td>\n",
       "      <td>1454305172</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smittyblack</td>\n",
       "      <td>Does he have a guy coming in early for himself...</td>\n",
       "      <td>SandersForPresident</td>\n",
       "      <td>1454306535</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                               body  \\\n",
       "0  Jesuslordofporn  If that is the case, I wonder if he would make...   \n",
       "1       2gurl2gurl  The key through getting legislation through Co...   \n",
       "2       2gurl2gurl  They win because we sit at home unless we are ...   \n",
       "3             jrrl  Or, better, spend those 30 minutes meeting oth...   \n",
       "4      Smittyblack  Does he have a guy coming in early for himself...   \n",
       "\n",
       "             subreddit  created_utc  score  \n",
       "0  SandersForPresident   1454297222      0  \n",
       "1  SandersForPresident   1454302379      5  \n",
       "2  SandersForPresident   1454302485      2  \n",
       "3  SandersForPresident   1454305172     11  \n",
       "4  SandersForPresident   1454306535      6  "
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
